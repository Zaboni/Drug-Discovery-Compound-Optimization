{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# üî¨ Advanced Molecular Feature Engineering\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook demonstrates advanced feature engineering techniques for molecular datasets.\\n\",\n",
    "    \"\\n\",\n",
    "    \"## üéØ Objectives\\n\",\n",
    "    \"- Extract comprehensive molecular descriptors\\n\",\n",
    "    \"- Generate various fingerprint representations\\n\",\n",
    "    \"- Perform feature selection and dimensionality reduction\\n\",\n",
    "    \"- Analyze feature importance and correlations\\n\",\n",
    "    \"- Create custom molecular features\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Import required libraries\\n\",\n",
    "    \"import sys\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"from pathlib import Path\\n\",\n",
    "    \"import warnings\\n\",\n",
    "    \"warnings.filterwarnings('ignore')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add src to path\\n\",\n",
    "    \"sys.path.append(str(Path.cwd().parent / 'src'))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Import project modules\\n\",\n",
    "    \"from data_processing import (\\n\",\n",
    "    \"    MolecularDataLoader, MolecularPreprocessor, \\n\",\n",
    "    \"    FeatureEnginerator, AdvancedFeatureExtractor\\n\",\n",
    "    \")\\n\",\n",
    "    \"from utils import (\\n\",\n",
    "    \"    calculate_molecular_descriptors, plot_molecular_properties,\\n\",\n",
    "    \"    plot_correlation_matrix, filter_molecules_by_properties\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Feature selection and ML imports\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    from sklearn.feature_selection import (\\n\",\n",
    "    \"        SelectKBest, f_regression, mutual_info_regression,\\n\",\n",
    "    \"        VarianceThreshold, SelectFromModel\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    from sklearn.decomposition import PCA\\n\",\n",
    "    \"    from sklearn.preprocessing import StandardScaler\\n\",\n",
    "    \"    from sklearn.ensemble import RandomForestRegressor\\n\",\n",
    "    \"    from sklearn.manifold import TSNE\\n\",\n",
    "    \"    from sklearn.cluster import KMeans\\n\",\n",
    "    \"    SKLEARN_AVAILABLE = True\\n\",\n",
    "    \"except ImportError:\\n\",\n",
    "    \"    SKLEARN_AVAILABLE = False\\n\",\n",
    "    \"    print(\\\"‚ö†Ô∏è  Scikit-learn not available. Some features will be limited.\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Set plotting style\\n\",\n",
    "    \"plt.style.use('seaborn-v0_8')\\n\",\n",
    "    \"sns.set_palette(\\\"husl\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"‚úÖ Libraries imported successfully!\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üìÅ Data Loading and Preparation\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's start by loading and preparing molecular data.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Initialize components\\n\",\n",
    "    \"loader = MolecularDataLoader()\\n\",\n",
    "    \"preprocessor = MolecularPreprocessor()\\n\",\n",
    "    \"feature_eng = FeatureEnginerator()\\n\",\n",
    "    \"advanced_features = AdvancedFeatureExtractor()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load data - check if processed data exists first\\n\",\n",
    "    \"processed_data_path = Path.cwd().parent / 'data' / 'processed' / 'processed_data.csv'\\n\",\n",
    "    \"\\n\",\n",
    "    \"if processed_data_path.exists():\\n\",\n",
    "    \"    print(f\\\"üìÇ Loading processed data from: {processed_data_path}\\\")\\n\",\n",
    "    \"    df = pd.read_csv(processed_data_path)\\n\",\n",
    "    \"    print(f\\\"‚úÖ Loaded {len(df)} records\\\")\\nelse:\\n\",\n",
    "    \"    # Check for raw data\\n\",\n",
    "    \"    data_dir = Path.cwd().parent / 'data' / 'raw'\\n\",\n",
    "    \"    sample_files = list(data_dir.glob('*.csv'))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if sample_files:\\n\",\n",
    "    \"        data_file = sample_files[0]\\n\",\n",
    "    \"        print(f\\\"üìÇ Loading raw data from: {data_file}\\\")\\n\",\n",
    "    \"        df_raw = loader.load_csv_file(str(data_file))\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Quick preprocessing\\n\",\n",
    "    \"        print(\\\"üßπ Quick preprocessing...\\\")\\n\",\n",
    "    \"        df = preprocessor.validate_molecules(df_raw, 'smiles')\\n\",\n",
    "    \"        df = preprocessor.standardize_molecules(df, 'smiles')\\n\",\n",
    "    \"        print(f\\\"‚úÖ Preprocessed {len(df)} records\\\")\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        # Create synthetic data\\n\",\n",
    "    \"        print(\\\"üìù Creating synthetic molecular dataset...\\\")\\n\",\n",
    "    \"        sample_data = {\\n\",\n",
    "    \"            'smiles': [\\n\",\n",
    "    \"                'CCO',  # Ethanol\\n\",\n",
    "    \"                'CC(=O)O',  # Acetic acid\\n\",\n",
    "    \"                'c1ccccc1',  # Benzene\\n\",\n",
    "    \"                'CC(C)CC1=CC=C(C=C1)C(C)C(=O)O',  # Ibuprofen\\n\",\n",
    "    \"                'CN1C=NC2=C1C(=O)N(C(=O)N2C)C',  # Caffeine\\n\",\n",
    "    \"                'CC(C)(C)NCC(C1=CC(=C(C=C1)O)CO)O',  # Salbutamol\\n\",\n",
    "    \"                'CC1=CC=C(C=C1)C(=O)O',  # p-Toluic acid\\n\",\n",
    "    \"                'C1=CC=C(C=C1)C(=O)O',  # Benzoic acid\\n\",\n",
    "    \"                'CC(C)NCC(C1=CC(=C(C=C1)O)CO)O',  # Albuterol\\n\",\n",
    "    \"                'C1CCC(CC1)N',  # Cyclohexylamine\\n\",\n",
    "    \"                'CCN(CC)CC(=O)NC1=C(C=CC(=C1)Cl)Cl',  # Lidocaine\\n\",\n",
    "    \"                'C1=CC=C2C(=C1)C(=CN2)CC(C(=O)O)N',  # Tryptophan\\n\",\n",
    "    \"                'CC(=O)NC1=CC=C(C=C1)O',  # Paracetamol\\n\",\n",
    "    \"                'CC(C)C1=CC=C(C=C1)C(C)C(=O)O'  # Another ibuprofen\\n\",\n",
    "    \"            ],\\n\",\n",
    "    \"            'name': [\\n\",\n",
    "    \"                'Ethanol', 'Acetic acid', 'Benzene', 'Ibuprofen', 'Caffeine',\\n\",\n",
    "    \"                'Salbutamol', 'p-Toluic acid', 'Benzoic acid', 'Albuterol',\\n\",\n",
    "    \"                'Cyclohexylamine', 'Lidocaine', 'Tryptophan', 'Paracetamol', 'Ibuprofen_2'\\n\",\n",
    "    \"            ],\\n\",\n",
    "    \"            'activity': [0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1],\\n\",\n",
    "    \"            'ic50_nM': [10000, 50000, 1000, 500, 2000, 750, 25000, 15000, 600, 8000, 300, 1200, 5000, 450],\\n\",\n",
    "    \"            'solubility': [0.8, 1.2, -0.5, -2.1, -0.3, 0.1, -1.8, -1.2, 0.2, -0.8, -2.5, -1.5, 0.5, -2.0]\\n\",\n",
    "    \"        }\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        df_raw = pd.DataFrame(sample_data)\\n\",\n",
    "    \"        df = preprocessor.validate_molecules(df_raw, 'smiles')\\n\",\n",
    "    \"        df = preprocessor.standardize_molecules(df, 'smiles')\\n\",\n",
    "    \"        print(f\\\"‚úÖ Created dataset with {len(df)} records\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Display basic info\\n\",\n",
    "    \"print(f\\\"\\\\nüìä Dataset Info:\\\")\\n\",\n",
    "    \"print(f\\\"Shape: {df.shape}\\\")\\n\",\n",
    "    \"print(f\\\"Columns: {list(df.columns)}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Show first few rows\\n\",\n",
    "    \"df.head()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üßÆ Basic Molecular Descriptors\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's start with basic molecular descriptors extraction.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Extract basic molecular descriptors\\n\",\n",
    "    \"print(\\\"üßÆ Extracting basic molecular descriptors...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Use canonical SMILES if available, otherwise use original SMILES\\n\",\n",
    "    \"smiles_col = 'canonical_smiles' if 'canonical_smiles' in df.columns else 'smiles'\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Extract descriptors\\n\",\n",
    "    \"df_descriptors = feature_eng.extract_molecular_descriptors(df, smiles_col)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Count new features\\n\",\n",
    "    \"new_descriptor_cols = [col for col in df_descriptors.columns if col not in df.columns]\\n\",\n",
    "    \"print(f\\\"‚úÖ Extracted {len(new_descriptor_cols)} molecular descriptors\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Display descriptor names\\n\",\n",
    "    \"print(f\\\"\\\\nüìù Molecular Descriptors:\\\")\\n\",\n",
    "    \"for i, desc in enumerate(new_descriptor_cols, 1):\\n\",\n",
    "    \"    print(f\\\"  {i:2d}. {desc}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Show sample data with descriptors\\n\",\n",
    "    \"if new_descriptor_cols:\\n\",\n",
    "    \"    print(f\\\"\\\\nüîç Sample data with descriptors:\\\")\\n\",\n",
    "    \"    sample_cols = ['name'] + new_descriptor_cols[:5]  # Show first 5 descriptors\\n\",\n",
    "    \"    display_df = df_descriptors[sample_cols].head()\\n\",\n",
    "    \"    print(display_df.round(3))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üî¢ Fingerprint Generation\\n\",\n",
    "    \"\\n\",\n",
    "    \"Now let's generate various molecular fingerprints.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Extract molecular fingerprints\\n\",\n",
    "    \"print(\\\"üî¢ Generating molecular fingerprints...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add fingerprints to our dataset\\n\",\n",
    "    \"df_fingerprints = feature_eng.extract_molecular_fingerprints(df_descriptors, smiles_col)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Count fingerprint features\\n\",\n",
    "    \"fingerprint_cols = [col for col in df_fingerprints.columns if col.startswith('fp_')]\\n\",\n",
    "    \"print(f\\\"‚úÖ Generated {len(fingerprint_cols)} fingerprint features\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Analyze fingerprint types\\n\",\n",
    "    \"fingerprint_types = {}\\n\",\n",
    "    \"for fp_col in fingerprint_cols:\\n\",\n",
    "    \"    if 'morgan' in fp_col:\\n\",\n",
    "    \"        fp_type = 'Morgan'\\n\",\n",
    "    \"    elif 'rdkit' in fp_col:\\n\",\n",
    "    \"        fp_type = 'RDKit'\\n\",\n",
    "    \"    elif 'maccs' in fp_col:\\n\",\n",
    "    \"        fp_type = 'MACCS'\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        fp_type = 'Other'\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    fingerprint_types[fp_type] = fingerprint_types.get(fp_type, 0) + 1\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nüìä Fingerprint breakdown:\\\")\\n\",\n",
    "    \"for fp_type, count in fingerprint_types.items():\\n\",\n",
    "    \"    print(f\\\"  {fp_type}: {count} features\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Analyze fingerprint density (sparsity)\\n\",\n",
    "    \"if fingerprint_cols:\\n\",\n",
    "    \"    fp_data = df_fingerprints[fingerprint_cols]\\n\",\n",
    "    \"    total_bits = fp_data.shape[0] * fp_data.shape[1]\\n\",\n",
    "    \"    set_bits = fp_data.sum().sum()\\n\",\n",
    "    \"    density = set_bits / total_bits * 100\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"\\\\nüéØ Fingerprint Statistics:\\\")\\n\",\n",
    "    \"    print(f\\\"  Total fingerprint bits: {len(fingerprint_cols)}\\\")\\n\",\n",
    "    \"    print(f\\\"  Bit density: {density:.2f}%\\\")\\n\",\n",
    "    \"    print(f\\\"  Sparsity: {100-density:.2f}%\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Show fingerprint usage distribution\\n\",\n",
    "    \"    bit_counts = fp_data.sum(axis=0)\\n\",\n",
    "    \"    print(f\\\"\\\\nüìà Bit usage statistics:\\\")\\n\",\n",
    "    \"    print(f\\\"  Min usage: {bit_counts.min()} molecules\\\")\\n\",\n",
    "    \"    print(f\\\"  Max usage: {bit_counts.max()} molecules\\\")\\n\",\n",
    "    \"    print(f\\\"  Mean usage: {bit_counts.mean():.2f} molecules\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Plot fingerprint bit usage distribution\\n\",\n",
    "    \"    plt.figure(figsize=(12, 5))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.subplot(1, 2, 1)\\n\",\n",
    "    \"    plt.hist(bit_counts, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\\n\",\n",
    "    \"    plt.title('Fingerprint Bit Usage Distribution')\\n\",\n",
    "    \"    plt.xlabel('Number of Molecules with Bit Set')\\n\",\n",
    "    \"    plt.ylabel('Number of Bits')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.subplot(1, 2, 2)\\n\",\n",
    "    \"    molecule_bit_counts = fp_data.sum(axis=1)\\n\",\n",
    "    \"    plt.hist(molecule_bit_counts, bins=20, alpha=0.7, color='lightgreen', edgecolor='black')\\n\",\n",
    "    \"    plt.title('Bits per Molecule Distribution')\\n\",\n",
    "    \"    plt.xlabel('Number of Bits Set per Molecule')\\n\",\n",
    "    \"    plt.ylabel('Number of Molecules')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.tight_layout()\\n\",\n",
    "    \"    plt.show()\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"‚ö†Ô∏è  No fingerprints generated\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Update our main dataframe\\n\",\n",
    "    \"df_features = df_fingerprints.copy()\\n\",\n",
    "    \"print(f\\\"\\\\nüìä Total features: {df_features.shape[1]} columns\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üéØ Advanced Feature Extraction\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's use the advanced feature extractor for more sophisticated features.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Extract advanced features\\n\",\n",
    "    \"print(\\\"üéØ Extracting advanced molecular features...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    # Extract pharmacophore features\\n\",\n",
    "    \"    print(\\\"\\\\n1Ô∏è‚É£ Pharmacophore features...\\\")\\n\",\n",
    "    \"    df_advanced = advanced_features.extract_pharmacophore_features(df_features, smiles_col)\\n\",\n",
    "    \"    pharmacophore_cols = [col for col in df_advanced.columns if col not in df_features.columns]\\n\",\n",
    "    \"    print(f\\\"   Added {len(pharmacophore_cols)} pharmacophore features\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Extract graph-based features\\n\",\n",
    "    \"    print(\\\"\\\\n2Ô∏è‚É£ Graph-based features...\\\")\\n\",\n",
    "    \"    df_advanced = advanced_features.extract_graph_features(df_advanced, smiles_col)\\n\",\n",
    "    \"    graph_cols = [col for col in df_advanced.columns if col not in df_features.columns and col not in pharmacophore_cols]\\n\",\n",
    "    \"    print(f\\\"   Added {len(graph_cols)} graph-based features\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Extract shape-based features\\n\",\n",
    "    \"    print(\\\"\\\\n3Ô∏è‚É£ Shape-based features...\\\")\\n\",\n",
    "    \"    df_advanced = advanced_features.extract_shape_features(df_advanced, smiles_col)\\n\",\n",
    "    \"    shape_cols = [col for col in df_advanced.columns if col not in df_features.columns and col not in pharmacophore_cols and col not in graph_cols]\\n\",\n",
    "    \"    print(f\\\"   Added {len(shape_cols)} shape-based features\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Update main dataframe\\n\",\n",
    "    \"    df_features = df_advanced.copy()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"\\\\n‚úÖ Advanced feature extraction completed!\\\")\\n\",\n",
    "    \"    print(f\\\"üìä Total features: {df_features.shape[1]} columns\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Show feature categories\\n\",\n",
    "    \"    all_feature_cols = [col for col in df_features.columns if col not in ['smiles', 'canonical_smiles', 'name', 'valid']]\\n\",\n",
    "    \"    descriptor_count = len([col for col in all_feature_cols if not col.startswith('fp_') and col not in ['activity', 'ic50_nM', 'solubility']])\\n\",\n",
    "    \"    fingerprint_count = len([col for col in all_feature_cols if col.startswith('fp_')])\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"\\\\nüìà Feature breakdown:\\\")\\n\",\n",
    "    \"    print(f\\\"  Molecular descriptors: {descriptor_count}\\\")\\n\",\n",
    "    \"    print(f\\\"  Fingerprint features: {fingerprint_count}\\\")\\n\",\n",
    "    \"    if pharmacophore_cols:\\n\",\n",
    "    \"        print(f\\\"  Pharmacophore features: {len(pharmacophore_cols)}\\\")\\n\",\n",
    "    \"    if graph_cols:\\n\",\n",
    "    \"        print(f\\\"  Graph features: {len(graph_cols)}\\\")\\n\",\n",
    "    \"    if shape_cols:\\n\",\n",
    "    \"        print(f\\\"  Shape features: {len(shape_cols)}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"except Exception as e:\\n\",\n",
    "    \"    print(f\\\"‚ö†Ô∏è  Advanced feature extraction failed: {e}\\\")\\n\",\n",
    "    \"    print(\\\"Continuing with basic features...\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üìä Feature Analysis and Visualization\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's analyze and visualize our extracted features.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Analyze feature distributions\\n\",\n",
    "    \"print(\\\"üìä Analyzing feature distributions...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Select numeric features (excluding identifiers and fingerprints)\\n\",\n",
    "    \"numeric_cols = df_features.select_dtypes(include=[np.number]).columns.tolist()\\n\",\n",
    "    \"descriptor_cols = [col for col in numeric_cols if not col.startswith('fp_') and col not in ['activity', 'ic50_nM', 'solubility']]\\n\",\n",
    "    \"\\n\",\n",
    "    \"if descriptor_cols:\\n\",\n",
    "    \"    print(f\\\"\\\\nüîç Analyzing {len(descriptor_cols)} numeric features\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Feature statistics\\n\",\n",
    "    \"    feature_stats = df_features[descriptor_cols].describe()\\n\",\n",
    "    \"    print(f\\\"\\\\nüìà Feature Statistics Summary:\\\")\\n\",\n",
    "    \"    print(feature_stats.round(3))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Visualize feature distributions\\n\",\n",
    "    \"    if len(descriptor_cols) >= 6:\\n\",\n",
    "    \"        # Select key features for visualization\\n\",\n",
    "    \"        key_features = descriptor_cols[:6]  # First 6 features\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        key_features = descriptor_cols\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if key_features:\\n\",\n",
    "    \"        print(f\\\"\\\\nüìä Visualizing distributions for key features...\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        n_features = len(key_features)\\n\",\n",
    "    \"        n_cols = min(3, n_features)\\n\",\n",
    "    \"        n_rows = (n_features + n_cols - 1) // n_cols\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4*n_rows))\\n\",\n",
    "    \"        if n_features == 1:\\n\",\n",
    "    \"            axes = [axes]\\n\",\n",
    "    \"        elif n_rows == 1:\\n\",\n",
    "    \"            axes = axes.flatten()\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            axes = axes.flatten()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        for i, feature in enumerate(key_features):\\n\",\n",
    "    \"            if i < len(axes):\\n\",\n",
    "    \"                data = df_features[feature].dropna()\\n\",\n",
    "    \"                axes[i].hist(data, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\\n\",\n",
    "    \"                axes[i].set_title(f'{feature.replace(\\\"_\\\", \\\" \\\").title()}')\\n\",\n",
    "    \"                axes[i].set_xlabel(feature.replace('_', ' '))\\n\",\n",
    "    \"                axes[i].set_ylabel('Frequency')\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                # Add statistics text\\n\",\n",
    "    \"                mean_val = data.mean()\\n\",\n",
    "    \"                std_val = data.std()\\n\",\n",
    "    \"                axes[i].axvline(mean_val, color='red', linestyle='--', alpha=0.7)\\n\",\n",
    "    \"                axes[i].text(0.7, 0.9, f'Œº={mean_val:.2f}\\\\nœÉ={std_val:.2f}', \\n\",\n",
    "    \"                            transform=axes[i].transAxes, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Hide unused subplots\\n\",\n",
    "    \"        for i in range(len(key_features), len(axes)):\\n\",\n",
    "    \"            axes[i].set_visible(False)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        plt.suptitle('Molecular Feature Distributions', fontsize=16)\\n\",\n",
    "    \"        plt.tight_layout()\\n\",\n",
    "    \"        plt.show()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Feature correlation analysis\\n\",\n",
    "    \"    print(f\\\"\\\\nüîó Feature correlation analysis...\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Calculate correlation matrix for a subset of features\\n\",\n",
    "    \"    if len(descriptor_cols) > 10:\\n\",\n",
    "    \"        # Use first 10 features for correlation analysis\\n\",\n",
    "    \"        corr_features = descriptor_cols[:10]\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        corr_features = descriptor_cols\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if len(corr_features) > 1:\\n\",\n",
    "    \"        corr_matrix = df_features[corr_features].corr()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Plot correlation heatmap\\n\",\n",
    "    \"        plt.figure(figsize=(12, 10))\\n\",\n",
    "    \"        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,\\n\",\n",
    "    \"                   square=True, linewidths=0.5, fmt='.2f',\\n\",\n",
    "    \"                   xticklabels=[col.replace('_', ' ') for col in corr_features],\\n\",\n",
    "    \"                   yticklabels=[col.replace('_', ' ') for col in corr_features])\\n\",\n",
    "    \"        plt.title('Feature Correlation Matrix')\\n\",\n",
    "    \"        plt.xticks(rotation=45, ha='right')\\n\",\n",
    "    \"        plt.yticks(rotation=0)\\n\",\n",
    "    \"        plt.tight_layout()\\n\",\n",
    "    \"        plt.show()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Find highly correlated feature pairs\\n\",\n",
    "    \"        high_corr_pairs = []\\n\",\n",
    "    \"        for i in range(len(corr_matrix.columns)):\\n\",\n",
    "    \"            for j in range(i+1, len(corr_matrix.columns)):\\n\",\n",
    "    \"                corr_val = corr_matrix.iloc[i, j]\\n\",\n",
    "    \"                if abs(corr_val) > 0.8:  # High correlation threshold\\n\",\n",
    "    \"                    high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_val))\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        if high_corr_pairs:\\n\",\n",
    "    \"            print(f\\\"\\\\n‚ö° Highly correlated feature pairs (|r| > 0.8):\\\")\\n\",\n",
    "    \"            for feat1, feat2, corr_val in high_corr_pairs:\\n\",\n",
    "    \"                print(f\\\"  {feat1} ‚Üî {feat2}: r = {corr_val:.3f}\\\")\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            print(f\\\"\\\\n‚úÖ No highly correlated feature pairs found (|r| > 0.8)\\\")\\n\",\n",
    "    \"            \\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"‚ö†Ô∏è  No numeric descriptor features found for analysis\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üéØ Feature Selection\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's apply various feature selection techniques to identify the most important features.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Feature Selection\\n\",\n",
    "    \"print(\\\"üéØ Performing feature selection...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"if SKLEARN_AVAILABLE and 'activity' in df_features.columns:\\n\",\n",
    "    \"    # Prepare data for feature selection\\n\",\n",
    "    \"    feature_cols = [col for col in df_features.columns \\n\",\n",
    "    \"                   if col not in ['smiles', 'canonical_smiles', 'name', 'valid', 'activity', 'ic50_nM', 'solubility']]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if len(feature_cols) > 0:\\n\",\n",
    "    \"        # Get feature matrix and target\\n\",\n",
    "    \"        X = df_features[feature_cols].copy()\\n\",\n",
    "    \"        y = df_features['activity'].copy()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Remove rows with missing target values\\n\",\n",
    "    \"        mask = ~y.isnull()\\n\",\n",
    "    \"        X = X[mask]\\n\",\n",
    "    \"        y = y[mask]\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Fill missing values with 0 (common for fingerprints)\\n\",\n",
    "    \"        X = X.fillna(0)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        if len(X) > 0:\\n\",\n",
    "    \"            print(f\\\"\\\\nüìä Feature selection data:\\\")\\n\",\n",
    "    \"            print(f\\\"  Samples: {X.shape[0]}\\\")\\n\",\n",
    "    \"            print(f\\\"  Features: {X.shape[1]}\\\")\\n\",\n",
    "    \"            print(f\\\"  Target distribution: {y.value_counts().to_dict()}\\\")\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # 1. Variance Threshold\\n\",\n",
    "    \"            print(f\\\"\\\\n1Ô∏è‚É£ Variance-based feature selection...\\\")\\n\",\n",
    "    \"            var_selector = VarianceThreshold(threshold=0.01)  # Remove features with very low variance\\n\",\n",
    "    \"            X_var = var_selector.fit_transform(X)\\n\",\n",
    "    \"            var_features = X.columns[var_selector.get_support()]\\n\",\n",
    "    \"            print(f\\\"   Selected {len(var_features)}/{len(feature_cols)} features (variance > 0.01)\\\")\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # 2. Univariate feature selection\\n\",\n",
    "    \"            print(f\\\"\\\\n2Ô∏è‚É£ Univariate feature selection...\\\")\\n\",\n",
    "    \"            if len(var_features) > 0:\\n\",\n",
    "    \"                k_best = min(20, len(var_features))  # Select top 20 features or all if less\\n\",\n",
    "    \"                univariate_selector = SelectKBest(score_func=f_regression, k=k_best)\\n\",\n",
    "    \"                X_univariate = univariate_selector.fit_transform(X[var_features], y)\\n\",\n",
    "    \"                univariate_features = var_features[univariate_selector.get_support()]\\n\",\n",
    "    \"                univariate_scores = univariate_selector.scores_[univariate_selector.get_support()]\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                print(f\\\"   Selected top {len(univariate_features)} features\\\")\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                # Show top features with scores\\n\",\n",
    "    \"                feature_scores = list(zip(univariate_features, univariate_scores))\\n\",\n",
    "    \"                feature_scores.sort(key=lambda x: x[1], reverse=True)\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                print(f\\\"\\\\nüèÜ Top 10 features by univariate score:\\\")\\n\",\n",
    "    \"                for i, (feature, score) in enumerate(feature_scores[:10], 1):\\n\",\n",
    "    \"                    print(f\\\"  {i:2d}. {feature}: {score:.3f}\\\")\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                # Visualize feature importance\\n\",\n",
    "    \"                if len(feature_scores) > 0:\\n\",\n",
    "    \"                    top_features = feature_scores[:min(15, len(feature_scores))]\\n\",\n",
    "    \"                    feature_names = [f[0] for f in top_features]\\n\",\n",
    "    \"                    scores = [f[1] for f in top_features]\\n\",\n",
    "    \"                    \\n\",\n",
    "    \"                    plt.figure(figsize=(12, 8))\\n\",\n",
    "    \"                    y_pos = np.arange(len(feature_names))\\n\",\n",
    "    \"                    plt.barh(y_pos, scores, alpha=0.7, color='skyblue')\\n\",\n",
    "    \"                    plt.yticks(y_pos, [name.replace('_', ' ') for name in feature_names])\\n\",\n",
    "    \"                    plt.xlabel('Univariate F-Score')\\n\",\n",
    "    \"                    plt.title('Top Features by Univariate Selection')\\n\",\n",
    "    \"                    plt.gca().invert_yaxis()\\n\",\n",
    "    \"                    plt.tight_layout()\\n\",\n",
    "    \"                    plt.show()\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # 3. Model-based feature selection\\n\",\n",
    "    \"            print(f\\\"\\\\n3Ô∏è‚É£ Model-based feature selection...\\\")\\n\",\n",
    "    \"            try:\\n\",\n",
    "    \"                if len(var_features) > 0:\\n\",\n",
    "    \"                    # Use Random Forest for feature importance\\n\",\n",
    "    \"                    rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\\n\",\n",
    "    \"                    rf.fit(X[var_features], y)\\n\",\n",
    "    \"                    \\n\",\n",
    "    \"                    # Get feature importances\\n\",\n",
    "    \"                    importances = rf.feature_importances_\\n\",\n",
    "    \"                    feature_importance = list(zip(var_features, importances))\\n\",\n",
    "    \"                    feature_importance.sort(key=lambda x: x[1], reverse=True)\\n\",\n",
    "    \"                    \\n\",\n",
    "    \"                    print(f\\\"\\\\nüå≥ Top 10 features by Random Forest importance:\\\")\\n\",\n",
    "    \"                    for i, (feature, importance) in enumerate(feature_importance[:10], 1):\\n\",\n",
    "    \"                        print(f\\\"  {i:2d}. {feature}: {importance:.4f}\\\")\\n\",\n",
    "    \"                    \\n\",\n",
    "    \"                    # Plot feature importance\\n\",\n",
    "    \"                    if len(feature_importance) > 0:\\n\",\n",
    "    \"                        top_rf_features = feature_importance[:min(15, len(feature_importance))]\\n\",\n",
    "    \"                        rf_feature_names = [f[0] for f in top_rf_features]\\n\",\n",
    "    \"                        rf_importances = [f[1] for f in top_rf_features]\\n\",\n",
    "    \"                        \\n\",\n",
    "    \"                        plt.figure(figsize=(12, 8))\\n\",\n",
    "    \"                        y_pos = np.arange(len(rf_feature_names))\\n\",\n",
    "    \"                        plt.barh(y_pos, rf_importances, alpha=0.7, color='lightgreen')\\n\",\n",
    "    \"                        plt.yticks(y_pos, [name.replace('_', ' ') for name in rf_feature_names])\\n\",\n",
    "    \"                        plt.xlabel('Feature Importance')\\n\",\n",
    "    \"                        plt.title('Top Features by Random Forest Importance')\\n\",\n",
    "    \"                        plt.gca().invert_yaxis()\\n\",\n",
    "    \"                        plt.tight_layout()\\n\",\n",
    "    \"                        plt.show()\\n\",\n",
    "    \"                        \\n\",\n",
    "    \"                        # Store selected features for later use\\n\",\n",
    "    \"                        selected_features = [f[0] for f in feature_importance[:20]]  # Top 20 features\\n\",\n",
    "    \"                        print(f\\\"\\\\n‚úÖ Selected {len(selected_features)} features for further analysis\\\")\\n\",\n",
    "    \"                    \\n\",\n",
    "    \"            except Exception as e:\\n\",\n",
    "    \"                print(f\\\"‚ö†Ô∏è  Model-based feature selection failed: {e}\\\")\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            print(f\\\"‚ö†Ô∏è  No valid samples for feature selection\\\")\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        print(f\\\"‚ö†Ô∏è  No features available for selection\\\")\\nelse:\\n\",\n",
    "    \"    print(f\\\"‚ö†Ô∏è  Feature selection skipped (sklearn not available or no target variable)\\\")\\n\",\n",
    "    \"    # Use all descriptor features\\n\",\n",
    "    \"    if descriptor_cols:\\n\",\n",
    "    \"        selected_features = descriptor_cols[:20]  # Use first 20 descriptor features\\n\",\n",
    "    \"        print(f\\\"Using {len(selected_features)} descriptor features for analysis\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üé® Dimensionality Reduction\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's apply dimensionality reduction techniques to visualize the feature space.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Dimensionality Reduction and Visualization\\n\",\n",
    "    \"print(\\\"üé® Applying dimensionality reduction...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"if SKLEARN_AVAILABLE and 'selected_features' in locals() and len(selected_features) > 1:\\n\",\n",
    "    \"    # Prepare data\\n\",\n",
    "    \"    X_selected = df_features[selected_features].copy()\\n\",\n",
    "    \"    X_selected = X_selected.fillna(0)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Standardize features\\n\",\n",
    "    \"    scaler = StandardScaler()\\n\",\n",
    "    \"    X_scaled = scaler.fit_transform(X_selected)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"\\\\nüìä Data for dimensionality reduction:\\\")\\n\",\n",
    "    \"    print(f\\\"  Samples: {X_scaled.shape[0]}\\\")\\n\",\n",
    "    \"    print(f\\\"  Features: {X_scaled.shape[1]}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 1. Principal Component Analysis (PCA)\\n\",\n",
    "    \"    print(f\\\"\\\\n1Ô∏è‚É£ Principal Component Analysis...\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Fit PCA\\n\",\n",
    "    \"    pca = PCA()\\n\",\n",
    "    \"    X_pca = pca.fit_transform(X_scaled)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Calculate explained variance\\n\",\n",
    "    \"    explained_variance_ratio = pca.explained_variance_ratio_\\n\",\n",
    "    \"    cumulative_variance = np.cumsum(explained_variance_ratio)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"   First 5 components explain: {cumulative_variance[4]*100:.1f}% of variance\\\")\\n\",\n",
    "    \"    print(f\\\"   First 10 components explain: {cumulative_variance[min(9, len(cumulative_variance)-1)]*100:.1f}% of variance\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Plot PCA results\\n\",\n",
    "    \"    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Explained variance plot\\n\",\n",
    "    \"    axes[0, 0].plot(range(1, len(explained_variance_ratio)+1), cumulative_variance, 'bo-')\\n\",\n",
    "    \"    axes[0, 0].axhline(y=0.8, color='r', linestyle='--', label='80% Variance')\\n\",\n",
    "    \"    axes[0, 0].axhline(y=0.9, color='g', linestyle='--', label='90% Variance')\\n\",\n",
    "    \"    axes[0, 0].set_xlabel('Number of Components')\\n\",\n",
    "    \"    axes[0, 0].set_ylabel('Cumulative Explained Variance')\\n\",\n",
    "    \"    axes[0, 0].set_title('PCA Explained Variance')\\n\",\n",
    "    \"    axes[0, 0].legend()\\n\",\n",
    "    \"    axes[0, 0].grid(True, alpha=0.3)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Scree plot\\n\",\n",
    "    \"    axes[0, 1].plot(range(1, min(21, len(explained_variance_ratio)+1)), \\n\",\n",
    "    \"                   explained_variance_ratio[:20], 'ro-')\\n\",\n",
    "    \"    axes[0, 1].set_xlabel('Principal Component')\\n\",\n",
    "    \"    axes[0, 1].set_ylabel('Explained Variance Ratio')\\n\",\n",
    "    \"    axes[0, 1].set_title('PCA Scree Plot (First 20 Components)')\\n\",\n",
    "    \"    axes[0, 1].grid(True, alpha=0.3)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # PCA scatter plot (PC1 vs PC2)\\n\",\n",
    "    \"    axes[1, 0].scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.7, s=60)\\n\",\n",
    "    \"    axes[1, 0].set_xlabel(f'PC1 ({explained_variance_ratio[0]*100:.1f}% variance)')\\n\",\n",
    "    \"    axes[1, 0].set_ylabel(f'PC2 ({explained_variance_ratio[1]*100:.1f}% variance)')\\n\",\n",
    "    \"    axes[1, 0].set_title('PCA: First Two Components')\\n\",\n",
    "    \"    axes[1, 0].grid(True, alpha=0.3)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Add activity coloring if available\\n\",\n",
    "    \"    if 'activity' in df_features.columns:\\n\",\n",
    "    \"        activity = df_features['activity']\\n\",\n",
    "    \"        mask = ~activity.isnull()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        if mask.sum() > 0:\\n\",\n",
    "    \"            axes[1, 0].clear()\\n\",\n",
    "    \"            scatter = axes[1, 0].scatter(X_pca[mask, 0], X_pca[mask, 1], \\n\",\n",
    "    \"                                       c=activity[mask], cmap='RdYlBu', alpha=0.7, s=60)\\n\",\n",
    "    \"            axes[1, 0].set_xlabel(f'PC1 ({explained_variance_ratio[0]*100:.1f}% variance)')\\n\",\n",
    "    \"            axes[1, 0].set_ylabel(f'PC2 ({explained_variance_ratio[1]*100:.1f}% variance)')\\n\",\n",
    "    \"            axes[1, 0].set_title('PCA: Colored by Activity')\\n\",\n",
    "    \"            axes[1, 0].grid(True, alpha=0.3)\\n\",\n",
    "    \"            plt.colorbar(scatter, ax=axes[1, 0], label='Activity')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # PCA scatter plot (PC2 vs PC3)\\n\",\n",
    "    \"    if X_pca.shape[1] > 2:\\n\",\n",
    "    \"        axes[1, 1].scatter(X_pca[:, 1], X_pca[:, 2], alpha=0.7, s=60)\\n\",\n",
    "    \"        axes[1, 1].set_xlabel(f'PC2 ({explained_variance_ratio[1]*100:.1f}% variance)')\\n\",\n",
    "    \"        axes[1, 1].set_ylabel(f'PC3 ({explained_variance_ratio[2]*100:.1f}% variance)')\\n\",\n",
    "    \"        axes[1, 1].set_title('PCA: Components 2 vs 3')\\n\",\n",
    "    \"        axes[1, 1].grid(True, alpha=0.3)\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        axes[1, 1].text(0.5, 0.5, 'Not enough components', ha='center', va='center')\\n\",\n",
    "    \"        axes[1, 1].set_title('PC2 vs PC3')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.suptitle('Principal Component Analysis Results', fontsize=16)\\n\",\n",
    "    \"    plt.tight_layout()\\n\",\n",
    "    \"    plt.show()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 2. t-SNE (if we have enough samples)\\n\",\n",
    "    \"    if X_scaled.shape[0] > 5:  # Need at least a few samples for t-SNE\\n\",\n",
    "    \"        print(f\\\"\\\\n2Ô∏è‚É£ t-SNE visualization...\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        try:\\n\",\n",
    "    \"            # Use first few PCA components for t-SNE to speed up computation\\n\",\n",
    "    \"            n_components_tsne = min(10, X_scaled.shape[1], X_scaled.shape[0]-1)\\n\",\n",
    "    \"            X_pca_reduced = X_pca[:, :n_components_tsne]\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Apply t-SNE\\n\",\n",
    "    \"            tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, X_scaled.shape[0]-1))\\n\",\n",
    "    \"            X_tsne = tsne.fit_transform(X_pca_reduced)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Plot t-SNE results\\n\",\n",
    "    \"            plt.figure(figsize=(12, 5))\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            plt.subplot(1, 2, 1)\\n\",\n",
    "    \"            plt.scatter(X_tsne[:, 0], X_tsne[:, 1], alpha=0.7, s=60)\\n\",\n",
    "    \"            plt.xlabel('t-SNE 1')\\n\",\n",
    "    \"            plt.ylabel('t-SNE 2')\\n\",\n",
    "    \"            plt.title('t-SNE Visualization')\\n\",\n",
    "    \"            plt.grid(True, alpha=0.3)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Color by activity if available\\n\",\n",
    "    \"            plt.subplot(1, 2, 2)\\n\",\n",
    "    \"            if 'activity' in df_features.columns:\\n\",\n",
    "    \"                activity = df_features['activity']\\n\",\n",
    "    \"                mask = ~activity.isnull()\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                if mask.sum() > 0:\\n\",\n",
    "    \"                    scatter = plt.scatter(X_tsne[mask, 0], X_tsne[mask, 1], \\n\",\n",
    "    \"                                        c=activity[mask], cmap='RdYlBu', alpha=0.7, s=60)\\n\",\n",
    "    \"                    plt.colorbar(scatter, label='Activity')\\n\",\n",
    "    \"                    plt.title('t-SNE: Colored by Activity')\\n\",\n",
    "    \"                else:\\n\",\n",
    "    \"                    plt.scatter(X_tsne[:, 0], X_tsne[:, 1], alpha=0.7, s=60)\\n\",\n",
    "    \"                    plt.title('t-SNE Visualization')\\n\",\n",
    "    \"            else:\\n\",\n",
    "    \"                plt.scatter(X_tsne[:, 0], X_tsne[:, 1], alpha=0.7, s=60)\\n\",\n",
    "    \"                plt.title('t-SNE Visualization')\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            plt.xlabel('t-SNE 1')\\n\",\n",
    "    \"            plt.ylabel('t-SNE 2')\\n\",\n",
    "    \"            plt.grid(True, alpha=0.3)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            plt.suptitle('t-SNE Dimensionality Reduction', fontsize=16)\\n\",\n",
    "    \"            plt.tight_layout()\\n\",\n",
    "    \"            plt.show()\\n\",\n",
    "    \"            \\n\",\n",
    "    \"        except Exception as e:\\n\",\n",
    "    \"            print(f\\\"‚ö†Ô∏è  t-SNE failed: {e}\\\")\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        print(f\\\"\\\\n‚ö†Ô∏è  Skipping t-SNE: need more samples (current: {X_scaled.shape[0]})\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 3. Feature clustering\\n\",\n",
    "    \"    print(f\\\"\\\\n3Ô∏è‚É£ Feature clustering analysis...\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        # Cluster features based on correlation\\n\",\n",
    "    \"        feature_corr = np.corrcoef(X_scaled.T)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Use correlation distance for clustering\\n\",\n",
    "    \"        from scipy.spatial.distance import squareform\\n\",\n",
    "    \"        from scipy.cluster.hierarchy import linkage, dendrogram\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Calculate distance matrix\\n\",\n",
    "    \"        distance_matrix = 1 - np.abs(feature_corr)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Perform hierarchical clustering\\n\",\n",
    "    \"        linkage_matrix = linkage(squareform(distance_matrix), method='ward')\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Plot dendrogram\\n\",\n",
    "    \"        plt.figure(figsize=(15, 8))\\n\",\n",
    "    \"        dendrogram(linkage_matrix, labels=[f.replace('_', ' ')[:15] for f in selected_features],\\n\",\n",
    "    \"                  leaf_rotation=90, leaf_font_size=8)\\n\",\n",
    "    \"        plt.title('Feature Clustering Dendrogram')\\n\",\n",
    "    \"        plt.ylabel('Distance')\\n\",\n",
    "    \"        plt.xlabel('Features')\\n\",\n",
    "    \"        plt.tight_layout()\\n\",\n",
    "    \"        plt.show()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"    except Exception as e:\\n\",\n",
    "    \"        print(f\\\"‚ö†Ô∏è  Feature clustering failed: {e}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(f\\\"‚ö†Ô∏è  Dimensionality reduction skipped (sklearn not available or insufficient features)\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üèóÔ∏è Custom Feature Engineering\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's create some custom features based on domain knowledge.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Custom Feature Engineering\\n\",\n",
    "    \"print(\\\"üèóÔ∏è Creating custom molecular features...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create a copy for custom features\\n\",\n",
    "    \"df_custom = df_features.copy()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# 1. Lipinski-based features\\n\",\n",
    "    \"print(f\\\"\\\\n1Ô∏è‚É£ Lipinski-based features...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    # Check if we have the required descriptors\\n\",\n",
    "    \"    required_descriptors = ['molecular_weight', 'logp', 'num_hbd', 'num_hba']\\n\",\n",
    "    \"    available_descriptors = [desc for desc in required_descriptors if desc in df_custom.columns]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if len(available_descriptors) == 4:\\n\",\n",
    "    \"        # Lipinski violations count\\n\",\n",
    "    \"        df_custom['lipinski_violations'] = (\\n\",\n",
    "    \"            (df_custom['molecular_weight'] > 500).astype(int) +\\n\",\n",
    "    \"            (df_custom['logp'] > 5).astype(int) +\\n\",\n",
    "    \"            (df_custom['num_hbd'] > 5).astype(int) +\\n\",\n",
    "    \"            (df_custom['num_hba'] > 10).astype(int)\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Drug-likeness score (inverse of violations normalized)\\n\",\n",
    "    \"        df_custom['drug_likeness_score'] = 1 - (df_custom['lipinski_violations'] / 4)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Molecular weight categories\\n\",\n",
    "    \"        df_custom['mw_category'] = pd.cut(df_custom['molecular_weight'], \\n\",\n",
    "    \"                                         bins=[0, 250, 500, 750, np.inf],\\n\",\n",
    "    \"                                         labels=['small', 'medium', 'large', 'very_large'])\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # LogP categories\\n\",\n",
    "    \"        df_custom['logp_category'] = pd.cut(df_custom['logp'],\\n\",\n",
    "    \"                                           bins=[-np.inf, 0, 2, 5, np.inf],\\n\",\n",
    "    \"                                           labels=['hydrophilic', 'moderate', 'lipophilic', 'very_lipophilic'])\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        print(f\\\"   Added Lipinski-based features: lipinski_violations, drug_likeness_score, mw_category, logp_category\\\")\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        print(f\\\"   Missing required descriptors: {set(required_descriptors) - set(available_descriptors)}\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"except Exception as e:\\n\",\n",
    "    \"    print(f\\\"   Failed to create Lipinski features: {e}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# 2. Complexity and diversity features\\n\",\n",
    "    \"print(f\\\"\\\\n2Ô∏è‚É£ Molecular complexity features...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    # Ring complexity\\n\",\n",
    "    \"    if 'num_aromatic_rings' in df_custom.columns and 'num_rings' in df_custom.columns:\\n\",\n",
    "    \"        df_custom['ring_complexity'] = df_custom['num_aromatic_rings'] + df_custom['num_rings']\\n\",\n",
    "    \"        df_custom['aromatic_fraction'] = df_custom['num_aromatic_rings'] / (df_custom['num_rings'] + 1e-6)\\n\",\n",
    "    \"        print(f\\\"   Added ring complexity features\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Heteroatom features\\n\",\n",
    "    \"    if 'num_heteroatoms' in df_custom.columns and 'num_heavy_atoms' in df_custom.columns:\\n\",\n",
    "    \"        df_custom['heteroatom_fraction'] = df_custom['num_heteroatoms'] / df_custom['num_heavy_atoms']\\n\",\n",
    "    \"        print(f\\\"   Added heteroatom fraction\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Flexibility features\\n\",\n",
    "    \"    if 'num_rotatable_bonds' in df_custom.columns and 'num_heavy_atoms' in df_custom.columns:\\n\",\n",
    "    \"        df_custom['flexibility_index'] = df_custom['num_rotatable_bonds'] / df_custom['num_heavy_atoms']\\n\",\n",
    "    \"        print(f\\\"   Added flexibility index\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Polarity features\\n\",\n",
    "    \"    if 'tpsa' in df_custom.columns and 'molecular_weight' in df_custom.columns:\\n\",\n",
    "    \"        df_custom['polarity_index'] = df_custom['tpsa'] / df_custom['molecular_weight']\\n\",\n",
    "    \"        print(f\\\"   Added polarity index\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"except Exception as e:\\n\",\n",
    "    \"    print(f\\\"   Failed to create complexity features: {e}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# 3. Interaction features\\n\",\n",
    "    \"print(f\\\"\\\\n3Ô∏è‚É£ Feature interactions...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    # Create interaction features between key descriptors\\n\",\n",
    "    \"    if 'molecular_weight' in df_custom.columns and 'logp' in df_custom.columns:\\n\",\n",
    "    \"        df_custom['mw_logp_ratio'] = df_custom['molecular_weight'] / (np.abs(df_custom['logp']) + 1)\\n\",\n",
    "    \"        print(f\\\"   Added MW/LogP ratio\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if 'num_hba' in df_custom.columns and 'num_hbd' in df_custom.columns:\\n\",\n",
    "    \"        df_custom['hb_balance'] = df_custom['num_hba'] - df_custom['num_hbd']\\n\",\n",
    "    \"        df_custom['total_hb'] = df_custom['num_hba'] + df_custom['num_hbd']\\n\",\n",
    "    \"        print(f\\\"   Added hydrogen bond features\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"except Exception as e:\\n\",\n",
    "    \"    print(f\\\"   Failed to create interaction features: {e}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Count new custom features\\n\",\n",
    "    \"new_custom_features = [col for col in df_custom.columns if col not in df_features.columns]\\n\",\n",
    "    \"print(f\\\"\\\\n‚úÖ Created {len(new_custom_features)} custom features\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"if new_custom_features:\\n\",\n",
    "    \"    print(f\\\"\\\\nüìù Custom features created:\\\")\\n\",\n",
    "    \"    for i, feature in enumerate(new_custom_features, 1):\\n\",\n",
    "    \"        print(f\\\"  {i:2d}. {feature}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Visualize some custom features\\n\",\n",
    "    \"    numeric_custom = [col for col in new_custom_features if df_custom[col].dtype in ['int64', 'float64']]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if len(numeric_custom) > 0:\\n\",\n",
    "    \"        print(f\\\"\\\\nüìä Visualizing custom features...\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        n_features = min(6, len(numeric_custom))\\n\",\n",
    "    \"        n_cols = min(3, n_features)\\n\",\n",
    "    \"        n_rows = (n_features + n_cols - 1) // n_cols\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4*n_rows))\\n\",\n",
    "    \"        if n_features == 1:\\n\",\n",
    "    \"            axes = [axes]\\n\",\n",
    "    \"        elif n_rows == 1:\\n\",\n",
    "    \"            axes = axes.flatten()\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            axes = axes.flatten()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        for i, feature in enumerate(numeric_custom[:n_features]):\\n\",\n",
    "    \"            if i < len(axes):\\n\",\n",
    "    \"                data = df_custom[feature].dropna()\\n\",\n",
    "    \"                axes[i].hist(data, bins=15, alpha=0.7, color='lightcoral', edgecolor='black')\\n\",\n",
    "    \"                axes[i].set_title(f'{feature.replace(\\\"_\\\", \\\" \\\").title()}')\\n\",\n",
    "    \"                axes[i].set_xlabel(feature.replace('_', ' '))\\n\",\n",
    "    \"                axes[i].set_ylabel('Frequency')\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                # Add statistics\\n\",\n",
    "    \"                mean_val = data.mean()\\n\",\n",
    "    \"                axes[i].axvline(mean_val, color='red', linestyle='--', alpha=0.7)\\n\",\n",
    "    \"                axes[i].text(0.7, 0.9, f'Œº={mean_val:.2f}', \\n\",\n",
    "    \"                            transform=axes[i].transAxes, \\n\",\n",
    "    \"                            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Hide unused subplots\\n\",\n",
    "    \"        for i in range(n_features, len(axes)):\\n\",\n",
    "    \"            axes[i].set_visible(False)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        plt.suptitle('Custom Feature Distributions', fontsize=16)\\n\",\n",
    "    \"        plt.tight_layout()\\n\",\n",
    "    \"        plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Update main dataframe with custom features\\n\",\n",
    "    \"df_features = df_custom.copy()\\n\",\n",
    "    \"print(f\\\"\\\\nüìä Final dataset shape: {df_features.shape}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üìã Feature Engineering Summary\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's summarize the feature engineering process and provide recommendations.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Feature Engineering Summary\\n\",\n",
    "    \"print(\\\"üìã FEATURE ENGINEERING SUMMARY\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*50)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Count different types of features\\n\",\n",
    "    \"all_cols = df_features.columns.tolist()\\n\",\n",
    "    \"identifier_cols = ['smiles', 'canonical_smiles', 'name', 'valid']\\n\",\n",
    "    \"target_cols = ['activity', 'ic50_nM', 'solubility']\\n\",\n",
    "    \"fingerprint_cols = [col for col in all_cols if col.startswith('fp_')]\\n\",\n",
    "    \"custom_cols = new_custom_features if 'new_custom_features' in locals() else []\\n\",\n",
    "    \"descriptor_cols = [col for col in all_cols if col not in identifier_cols + target_cols + fingerprint_cols + custom_cols]\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nüìä Feature Breakdown:\\\")\\n\",\n",
    "    \"print(f\\\"  Total columns: {len(all_cols)}\\\")\\n\",\n",
    "    \"print(f\\\"  Molecular descriptors: {len(descriptor_cols)}\\\")\\n\",\n",
    "    \"print(f\\\"  Fingerprint features: {len(fingerprint_cols)}\\\")\\n\",\n",
    "    \"print(f\\\"  Custom features: {len(custom_cols)}\\\")\\n\",\n",
    "    \"print(f\\\"  Target variables: {len([col for col in target_cols if col in all_cols])}\\\")\\n\",\n",
    "    \"print(f\\\"  Identifier columns: {len([col for col in identifier_cols if col in all_cols])}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Feature quality assessment\\n\",\n",
    "    \"print(f\\\"\\\\nüîç Feature Quality Assessment:\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Check for missing values\\n\",\n",
    "    \"feature_cols = descriptor_cols + fingerprint_cols + custom_cols\\n\",\n",
    "    \"if feature_cols:\\n\",\n",
    "    \"    missing_counts = df_features[feature_cols].isnull().sum()\\n\",\n",
    "    \"    features_with_missing = missing_counts[missing_counts > 0]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if len(features_with_missing) > 0:\\n\",\n",
    "    \"        print(f\\\"  Features with missing values: {len(features_with_missing)}\\\")\\n\",\n",
    "    \"        if len(features_with_missing) <= 5:\\n\",\n",
    "    \"            for feature, count in features_with_missing.items():\\n\",\n",
    "    \"                print(f\\\"    {feature}: {count} missing\\\")\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        print(f\\\"  ‚úÖ No missing values in features\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Check for constant features\\n\",\n",
    "    \"    numeric_features = df_features[feature_cols].select_dtypes(include=[np.number]).columns\\n\",\n",
    "    \"    if len(numeric_features) > 0:\\n\",\n",
    "    \"        feature_std = df_features[numeric_features].std()\\n\",\n",
    "    \"        constant_features = feature_std[feature_std == 0]\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        if len(constant_features) > 0:\\n\",\n",
    "    \"            print(f\\\"  ‚ö†Ô∏è  Constant features found: {len(constant_features)}\\\")\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            print(f\\\"  ‚úÖ No constant features\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Check feature ranges\\n\",\n",
    "    \"    if len(numeric_features) > 0:\\n\",\n",
    "    \"        feature_ranges = df_features[numeric_features].max() - df_features[numeric_features].min()\\n\",\n",
    "    \"        large_range_features = feature_ranges[feature_ranges > 1000]\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        if len(large_range_features) > 0:\\n\",\n",
    "    \"            print(f\\\"  ‚ö†Ô∏è  Features with large ranges: {len(large_range_features)} (consider scaling)\\\")\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            print(f\\\"  ‚úÖ Feature ranges are reasonable\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Recommendations\\n\",\n",
    "    \"print(f\\\"\\\\nüí° Recommendations:\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Basic recommendations\\n\",\n",
    "    \"recommendations = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"if len(fingerprint_cols) > 1000:\\n\",\n",
    "    \"    recommendations.append(\\\"Consider dimensionality reduction for fingerprint features\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"if len(feature_cols) > 50:\\n\",\n",
    "    \"    recommendations.append(\\\"Apply feature selection before model training\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"if 'selected_features' in locals():\\n\",\n",
    "    \"    recommendations.append(f\\\"Use the {len(selected_features)} selected features for initial modeling\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"if len(custom_cols) > 0:\\n\",\n",
    "    \"    recommendations.append(\\\"Validate custom features with domain experts\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"if SKLEARN_AVAILABLE:\\n\",\n",
    "    \"    recommendations.append(\\\"Consider ensemble feature selection methods\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    recommendations.append(\\\"Install scikit-learn for advanced feature selection\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# General recommendations\\n\",\n",
    "    \"recommendations.extend([\\n\",\n",
    "    \"    \\\"Standardize features before training models\\\",\\n\",\n",
    "    \"    \\\"Consider feature interactions for non-linear models\\\",\\n\",\n",
    "    \"    \\\"Validate feature importance with multiple algorithms\\\",\\n\",\n",
    "    \"    \\\"Monitor for feature drift in production\\\"\\n\",\n",
    "    \"])\\n\",\n",
    "    \"\\n\",\n",
    "    \"for i, rec in enumerate(recommendations, 1):\\n\",\n",
    "    \"    print(f\\\"  {i}. {rec}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Next steps\\n\",\n",
    "    \"print(f\\\"\\\\nüöÄ Next Steps:\\\")\\n\",\n",
    "    \"print(f\\\"  1. Save processed features for model training\\\")\\n\",\n",
    "    \"print(f\\\"  2. Split data using molecular scaffolds or clusters\\\")\\n\",\n",
    "    \"print(f\\\"  3. Build baseline models with current features\\\")\\n\",\n",
    "    \"print(f\\\"  4. Analyze feature importance in model context\\\")\\n\",\n",
    "    \"print(f\\\"  5. Iterate on feature engineering based on model performance\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"if 'activity' in df_features.columns:\\n\",\n",
    "    \"    print(f\\\"  6. Build classification models for activity prediction\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"if 'ic50_nM' in df_features.columns or 'solubility' in df_features.columns:\\n\",\n",
    "    \"    print(f\\\"  7. Build regression models for continuous targets\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\n‚úÖ Feature engineering completed successfully!\\\")\\n\",\n",
    "    \"print(f\\\"üìÅ Ready for model training and evaluation.\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Display final feature summary\\n\",\n",
    "    \"if feature_cols:\\n\",\n",
    "    \"    print(f\\\"\\\\nüìä Final Dataset Summary:\\\")\\n\",\n",
    "    \"    print(f\\\"  Rows: {df_features.shape[0]}\\\")\\n\",\n",
    "    \"    print(f\\\"  Total features: {len(feature_cols)}\\\")\\n\",\n",
    "    \"    print(f\\\"  Memory usage: {df_features.memory_usage(deep=True).sum() / 1024**2:.2f} MB\\\")\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.10.0\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}"
   ],
   "id": "fd4ec1a394941c0a"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
